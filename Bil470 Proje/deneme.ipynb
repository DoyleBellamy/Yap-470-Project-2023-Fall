{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from joblib import load\n",
    "import pickle\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Big_Graphs():\n",
    "    num_nodes = 2000\n",
    "    BigList_2000 =[]\n",
    "    random_complete = nx.complete_graph(2000)\n",
    "    for i in range(2000):\n",
    "        for j in range(i + 1, 2000):\n",
    "            if random.random() < 0.5:\n",
    "                random_complete.remove_edge(i, j)\n",
    "    BigList_2000.append(random_complete)\n",
    "    #-------------------------------------------------\n",
    "    random_multigraph = nx.MultiGraph()\n",
    "    for i in range(num_nodes):\n",
    "        random_multigraph.add_node(i)\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if random.random() < 0.5:\n",
    "                random_multigraph.add_edge(i, j)\n",
    "    BigList_2000.append(random_multigraph)\n",
    "    #-------------------------------------------------\n",
    "    pseudograph = nx.MultiGraph()\n",
    "    pseudograph.add_nodes_from(range(num_nodes))\n",
    "    for node in pseudograph.nodes():\n",
    "        pseudograph.add_edge(node, node)\n",
    "        random_nodes = random.sample(pseudograph.nodes(), k=3)\n",
    "        pseudograph.add_edges_from([(node, random_node) for random_node in random_nodes])\n",
    "    BigList_2000.append(pseudograph)\n",
    "    #-------------------------------------------------\n",
    "    planar_graph = nx.random_geometric_graph(num_nodes, radius=0.2)\n",
    "    while not nx.is_connected(planar_graph):\n",
    "        planar_graph = nx.random_geometric_graph(num_nodes, radius=0.2)\n",
    "    BigList_2000.append(planar_graph)\n",
    "    #-------------------------------------------------\n",
    "    def random_hamiltonian_graph(n):\n",
    "        graph = nx.random_regular_graph(2, n)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if random.random() < 0.5 and not graph.has_edge(i, j):\n",
    "                    graph.add_edge(i, j)\n",
    "        return graph\n",
    "    random_hamiltonian_graph = random_hamiltonian_graph(num_nodes)\n",
    "    BigList_2000.append(random_hamiltonian_graph)\n",
    "    return BigList_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_graphs(num_nodes):\n",
    "    main_list =[]\n",
    "   \n",
    "    random_regular_3 = nx.random_regular_graph(3, num_nodes)\n",
    "    random_regular_20 = nx.random_regular_graph(20, num_nodes)\n",
    "    main_list.append(random_regular_3)\n",
    "    main_list.append(random_regular_20)\n",
    "    #-------------------------------------------------\n",
    "    # Create a random connected graph by generating a random spanning tree\n",
    "    random_connected_tree = nx.random_tree(num_nodes)\n",
    "    # Add additional edges to maintain connectivity\n",
    "    while not nx.is_connected(random_connected_tree):\n",
    "        node1 = random.choice(list(random_connected_tree.nodes()))\n",
    "        node2 = random.choice(list(random_connected_tree.nodes()))\n",
    "        if node1 != node2 and not random_connected_tree.has_edge(node1, node2):\n",
    "            random_connected_tree.add_edge(node1, node2)\n",
    "    main_list.append(random_connected_tree)\n",
    "    #-------------------------------------------------\n",
    "    def generate_random_connected_cycle_graph(num_nodes):\n",
    "        if num_nodes < 3:\n",
    "            raise ValueError(\"Number of nodes must be at least 3 for a cycle.\")\n",
    "        cycle_graph = nx.cycle_graph(num_nodes)\n",
    "        # Connect the cycle to form a connected cycle graph\n",
    "        connected_cycle_graph = nx.connected_watts_strogatz_graph(num_nodes, 2, 0.1)\n",
    "        return connected_cycle_graph\n",
    "    random_cycle_graph = generate_random_connected_cycle_graph(num_nodes)\n",
    "    main_list.append(random_cycle_graph)\n",
    "    #-------------------------------------------------\n",
    "    def generate_random_connected_barabasi_albert_graph(num_nodes, m):\n",
    "        if num_nodes <= m:\n",
    "            raise ValueError(\"Number of nodes must be greater than m for a Barabási-Albert graph.\")\n",
    "        ba_graph = nx.barabasi_albert_graph(num_nodes, m)\n",
    "        while not nx.is_connected(ba_graph):\n",
    "            # Add edges to connect the graph\n",
    "            non_edges = list(nx.non_edges(ba_graph))\n",
    "            edge_to_add = non_edges[0]\n",
    "            ba_graph.add_edge(*edge_to_add)\n",
    "        return ba_graph\n",
    "    m_parameter = 2\n",
    "    random_connected_ba_graph = generate_random_connected_barabasi_albert_graph(num_nodes, m_parameter)\n",
    "    main_list.append(random_connected_ba_graph)\n",
    "    #-------------------------------------------------\n",
    "   \n",
    "    '''\n",
    "    def generate_random_connected_erdos_renyi_graph(num_nodes, probability):\n",
    "        er_graph = nx.erdos_renyi_graph(num_nodes, probability)\n",
    "        while not nx.is_connected(er_graph):\n",
    "            # Add edges to connect the graph\n",
    "            non_edges = list(nx.non_edges(er_graph))\n",
    "            edge_to_add = non_edges[0]\n",
    "            er_graph.add_edge(*edge_to_add)\n",
    "        return er_graph\n",
    "    edge_probability = 0.1  # Adjust as needed\n",
    "    random_connected_er_graph = generate_random_connected_erdos_renyi_graph(num_nodes, edge_probability)\n",
    "    main_list.append(random_connected_er_graph)\n",
    "    '''\n",
    "    #-------------------------------------------------\n",
    "    k = 2  # Each node is connected to k nearest neighbors\n",
    "    p = 0.3  # Probability of rewiring each edge\n",
    "    random_graph_ws = nx.watts_strogatz_graph(num_nodes, k, p)\n",
    "    main_list.append(random_graph_ws)\n",
    "    #-------------------------------------------------\n",
    "    k = 10  # Each node is connected to k nearest neighbors\n",
    "    p = 0.3  # Probability of rewiring each edge\n",
    "    random_graph_ws = nx.watts_strogatz_graph(num_nodes, k, p)\n",
    "    main_list.append(random_graph_ws)\n",
    "    #-------------------------------------------------\n",
    "\n",
    "    def generate_connected_random_geometric_graph(num_nodes, radius):\n",
    "        random_geo_graph = nx.random_geometric_graph(num_nodes, radius)\n",
    "        while not nx.is_connected(random_geo_graph):\n",
    "            # Connect the graph by adding edges\n",
    "            non_edges = list(nx.non_edges(random_geo_graph))\n",
    "            random_edge = random.choice(non_edges)\n",
    "            random_geo_graph.add_edge(*random_edge)\n",
    "        return random_geo_graph\n",
    "    radius = 0.1  # Adjust as needed\n",
    "    connected_random_geo_graph = generate_connected_random_geometric_graph(num_nodes, radius)\n",
    "    main_list.append(connected_random_geo_graph)\n",
    "    #-------------------------------------------------\n",
    "    def generate_connected_random_internet_graph(num_nodes, k, p):\n",
    "        internet_graph = nx.random_internet_as_graph(num_nodes)\n",
    "        while not nx.is_connected(internet_graph):\n",
    "            non_edges = list(nx.non_edges(internet_graph))\n",
    "            random_edge = non_edges[0]\n",
    "            internet_graph.add_edge(*random_edge)\n",
    "        undirected_internet_graph = internet_graph.to_undirected()\n",
    "        connected_random_internet_graph = nx.connected_watts_strogatz_graph(num_nodes, k, p)\n",
    "        return connected_random_internet_graph\n",
    "    k_parameter = 5\n",
    "    p_parameter = 0.5\n",
    "    connected_random_internet_graph = generate_connected_random_internet_graph(num_nodes, k_parameter, p_parameter)\n",
    "    main_list.append(connected_random_internet_graph)\n",
    "    #-------------------------------------------------\n",
    "    return main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigGraphs = create_Big_Graphs()\n",
    "main_list = create_main_graphs(5000)\n",
    "main_list2 = create_main_graphs(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernighan_lin import kernighan_lin_bisection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KernighanLinIterationAndEmbedding(G):\n",
    "    \n",
    "    total_vertices = G.number_of_nodes()\n",
    "    \n",
    "    # determine total number of iterations\n",
    "    if total_vertices < 360:\n",
    "        totalNumberOfIteration = 10\n",
    "    elif total_vertices > 500:\n",
    "        totalNumberOfIteration = 0.4 * total_vertices * np.log10(total_vertices)\n",
    "    else:\n",
    "        totalNumberOfIteration = total_vertices * np.log10(total_vertices)\n",
    "    \n",
    "    totalNumberOfIteration = int(totalNumberOfIteration)\n",
    "    \n",
    "    #for j in range(totalNumberOfIteration):\n",
    "    partition = kernighan_lin_bisection(G, max_iter=totalNumberOfIteration)\n",
    "    \n",
    "    G_partition1 = G.subgraph(partition[0])\n",
    "    G_partition2 = G.subgraph(partition[1])\n",
    "    \n",
    "    if nx.is_connected(G_partition1) and nx.is_connected(G_partition2):\n",
    "        \n",
    "        partition_1_vertices = G_partition1.number_of_nodes()\n",
    "        partition_2_vertices = G_partition2.number_of_nodes()\n",
    "        \n",
    "        min_vertex_bound = total_vertices/2 - total_vertices*0.01\n",
    "        max_vertex_bound = total_vertices/2 + total_vertices*0.01\n",
    "        \n",
    "        if ((min_vertex_bound <= partition_1_vertices <= max_vertex_bound)\n",
    "                and (min_vertex_bound <= partition_2_vertices <= max_vertex_bound)):\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import random as rand\n",
    "import numpy as np\n",
    "# Graph Representation & Embedding Library\n",
    "import networkx as nx \n",
    "from node2vec import Node2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(G):\n",
    "    # Precompute probabilities and generate walks\n",
    "    node2vec = Node2Vec(G, dimensions=30, walk_length=6, num_walks=10, workers=6)\n",
    "\n",
    "    # Embed nodes\n",
    "    model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "\n",
    "    # Get embeddings for all nodes in the graph\n",
    "    all_node_embeddings = {node: model.wv[str(node)] for node in G.nodes()}\n",
    "\n",
    "    return all_node_embeddings\n",
    "    # loaded_model = Node2Vec.load(\"node2vec_model.bin\")\n",
    "    # loaded_embedding = loaded_model.wv['0']\n",
    "    # print(f\"Loaded Embedding for Node 0: {loaded_embedding}\")\n",
    "\n",
    "def dictionaryToNpArray(embedding):\n",
    "    array_from_dict = np.array(list(embedding.values()))\n",
    "    return array_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scriptKullanilacakFonksiyon(G):\n",
    "    \n",
    "    # Modeli yükleyip kullanma\n",
    "    # K-Means modelini yükle\n",
    "    with open('logistic_regression_model.pickle', 'rb') as model_file:\n",
    "        loaded_classifier = pickle.load(model_file)\n",
    "\n",
    "    nodeEmbeddings = getEmbedding(G) \n",
    "    nodeEmbeddingsArray = dictionaryToNpArray(nodeEmbeddings)\n",
    "    graphEmbedding = np.mean(nodeEmbeddingsArray, axis=0).astype('float')\n",
    "\n",
    "    result_prediction = loaded_classifier.predict(graphEmbedding.reshape(1, -1))\n",
    "    \n",
    "    # Prediction 0.5'den buyukse 1 veriyor (Yani partition edilebilir)\n",
    "    if result_prediction > 0.5:\n",
    "        return 1\n",
    "    \n",
    "    # Prediction 0.5'den kucukse 0 veriyor (Yani partition edilemedi)\n",
    "    else: \n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:00<00:00, 21200.77it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:09<00:00, 876.16it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:00<00:00, 29938.14it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:00<00:00, 36052.39it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:01<00:00, 6374.15it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:00<00:00, 34594.94it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [00:02<00:00, 3416.94it/s]\n",
      "c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.3.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "Computing transition probabilities: 100%|██████████| 8000/8000 [18:04<00:00,  7.38it/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 155, in _feed\n    send_bytes(obj_)\n  File \"c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\multiprocessing\\connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"c:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\multiprocessing\\connection.py\", line 280, in _send_bytes\n    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)\nOSError: [WinError 1450] Insufficient system resources exist to complete the requested service\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-4253e6bce762>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmain_list2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mres_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscriptKullanilacakFonksiyon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-48981386683b>\u001b[0m in \u001b[0;36mscriptKullanilacakFonksiyon\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mloaded_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mnodeEmbeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mnodeEmbeddingsArray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionaryToNpArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodeEmbeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mgraphEmbedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodeEmbeddingsArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-76deee138e84>\u001b[0m in \u001b[0;36mgetEmbedding\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Precompute probabilities and generate walks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnode2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Embed nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\node2vec\\node2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, dimensions, walk_length, num_walks, p, q, weight_key, workers, sampling_strategy, quiet, temp_folder, seed)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_precompute_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_walks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_precompute_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\node2vec\\node2vec.py\u001b[0m in \u001b[0;36m_generate_walks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m                                              self.quiet) for\n\u001b[0;32m    160\u001b[0m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             in enumerate(num_walks_lists, 1))\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mwalks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bora\\anaconda3\\envs\\test1\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "label_val = []\n",
    "\n",
    "print(len(main_list2))\n",
    "\n",
    "for graph in main_list2:\n",
    "    label_val.append(KernighanLinIterationAndEmbedding(graph))\n",
    "\n",
    "res_val = []\n",
    "\n",
    "for graph in main_list2:\n",
    "        res_val.append(scriptKullanilacakFonksiyon(graph))\n",
    "    \n",
    "count = 0\n",
    "\n",
    "for i in range(len(res_val)):\n",
    "    if res_val[i] == label_val[i]:\n",
    "        count += 1\n",
    "\n",
    "print(count)\n",
    "print(len(res_val))\n",
    "print(float(count)/len(res_val))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
